* Hi, this is the rmd file for project 2. We will be doing logistic regression to model food insecurity



```{r, echo=FALSE, cache=TRUE }
Food_Sec <- data.frame(read.csv("dec21pub.csv"))
```



```{r, echo=FALSE, results='hide', message=FALSE}
library(dplyr)
library(ezids)
library(ggplot2)
library(epiR)
library(pROC)
library(smotefamily)
library(ROSE)
library(ggthemes)
```

* Lets start with the variables we selected for the first project. We should replace some variables I think, but we'll do it along the way. I am just copying code from the first project for now. so we can get a base to build upon.

```{r, results='markup', echo=FALSE}


FS_Subset <- subset(Food_Sec, HRINTSTA == 001 & HRSUPINT == 001 & HRFS12MD != -9)
FS_Subset <- subset(FS_Subset, select = c(	"GESTFIPS",	"HRNUMHOU",	"HEFAMINC",	"HESP1",	"PTDTRACE",	"PRCITSHP",	"PEMJNUM",	"PEHRUSL1",	"PEEDUCA", "PRNMCHLD" , "HRFS12MD"))

  

FS_Subset <- FS_Subset %>% rename("States" = "GESTFIPS", "Family_Size" = "HRNUMHOU",	"Household_Income" = "HEFAMINC",	"SNAP" = "HESP1",	"Ethnicity" =	"PTDTRACE", "Citizenship_status" = "PRCITSHP",	"Number_of_Jobs" = "PEMJNUM",	"Hours_on_Jobs" = "PEHRUSL1" , "Education_Level" = "PEEDUCA" , "Number_of_children" = "PRNMCHLD",  "FoodSecurity_score" = "HRFS12MD")



## Converting the all the columns to factors as they are all ordinal(except the Id, but since it's categorical i'm converting it into a factor too)


FS_Subset[] <- lapply( FS_Subset, factor)

str(FS_Subset)

```



* Since we will be doing logistic regression, we have to reduce our response to two levels(binary). If we complete this project before the deadline we can look into multinomial regression as well.




```{r, results='markup', echo=FALSE, results='hide'}
levels(FS_Subset$'FoodSecurity_score') <- c( "High Food Security", "Marginal Food Security", "Low Food Security", "Very Low Food Security")


FS_Subset$FS_Status <- FS_Subset$FoodSecurity_score

levels(FS_Subset$FS_Status) <- c( "Food Secure", "Food Secure", "Food Insecure", "Food Insecure")

levels(FS_Subset$'Ethnicity') <- c('White only', 'Black only', 'American Indian, Alaskan native only', 'Asian Only', 'Hawaiian', 'White-black', 'White-AI', 'White-Asian', 'White-HP', 'Black-AI', 'Black-Asian', 'Black-HP', 'AI-Asian', 'AI-HP', 'Asian-HP', 'W-B-AI', 'W-B-A', 'W-B-HP', 'W-AI-A', 'W-AI-HP', 'W-A-HP', 'B-AI-A', 'W-B-AL-A', 'W-AI-A-HP', 'Other 3 race combo', 'Other 4 and 5 race combo')






levels(FS_Subset$'Citizenship_status') <- c('NATIVE, BORN IN THE UNITED STATES', 'NATIVE, BORN IN PUERTO RICO OR OTHER U.S. ISLAND AREAS', 'NATIVE, BORN ABROAD OF AMERICAN PARENT OR PARENTS', 'FOREIGN BORN, U.S. CITIZEN BY NATURALIZATION', 'FOREIGN BORN, NOT A CITIZEN OF THE UNITED STATES')
summary(FS_Subset$'Citizenship_status', title = "PRCITSHP")







```

* Trying to clean the data ang get the factors like ethnicity to work.

```{r, results='markup'}
table(FS_Subset$Ethnicity)

## Lets drop all levels that have less than 10 observations.

for(i in levels(FS_Subset$Ethnicity)) {
  if(count(subset(FS_Subset, Ethnicity == i)) < 10){
    print(i)
    FS_Subset <- FS_Subset[!(FS_Subset$Ethnicity %in% c(i)), ]
  }}

FS_Subset$Ethnicity <- droplevels(FS_Subset$Ethnicity)
     
table(FS_Subset$Ethnicity)
```




* Splitting the data




```{r, results ='markup'}
set.seed(1)

sample <- sample(c(TRUE, FALSE), nrow(FS_Subset), replace = TRUE, prob = c(0.6, 0.4))

train <- FS_Subset[sample, ]
test  <- FS_Subset[!sample, ]

str(test)
str(train)
```

* Just adding all the variables in the in the logistic regression to see what happens.We can then take out some predictors and add others or add interaction terms. This will also serve as sample code if you all are stuck somewhere.


```{r, results='markup'}
logistic <- glm(FS_Status ~ Ethnicity + States + Family_Size + Household_Income:SNAP + Citizenship_status + Number_of_Jobs + Education_Level   , data = train, family = "binomial")



```

* predicting on the testing data and finding the mean squared error for this model.

```{r, results='markup'}

logistic_model1.prob <- predict(logistic, test, type = "response")
logistic_model1.pred = rep("Food Secure", dim(test)[1])
logistic_model1.pred[logistic_model1.prob > .5] = "Food Insecure"


table(logistic_model1.pred, test$FS_Status)
```
* Our precision for this model is `r  166/( 166+145)`

* Our recall for this model is `r  166/( 166+2642)`

* Our precision for this model is `r  185/( 185+180)`

* Our recall for this model is `r  185/( 185+2623)`

* In my opinion, recall is more important to us as we want to get our false negative down as much as possible. False positives are inclusion errors, while false negatives are exclusion errors. It's better if we are able to more accurately identify who all are food insecure and need of aid rather than identifying who all are not food insecure and hence don't need aid.

```{r, results='markup'}
train$logistic_model1_train.prob <- predict(logistic, train, type = "response")

# distribution of the prediction score grouped by known outcome
ggplot( train, aes( logistic_model1_train.prob, color = as.factor(FS_Status) ) ) + 
geom_density( size = 1 ) +
ggtitle( "Training Set's Predicted Score" ) + 
scale_color_economist( name = "data", labels = c( "Food Secure", "Food Insecure" ) ) + 
theme_economist()



```

```{r, results='markup'}
test$logistic_model1.prob <- logistic_model1.prob
roc_model1 <- roc(FS_Status ~ logistic_model1.prob, data = test)
plot(roc_model1)
auc(roc_model1)

```

```{r results='markup'}
sum(test$FS_Status == "Food Secure")/sum(test$FS_Status == "Food Insecure")

```

* So, our model has a 90% classification rate but the ratio of Food secure to Insecure in our data is 9.04. so this is a terrible model that's as good as tossing a coin. We will have to do something about the imbalance in our response and will have to clean our data further. Since all our variables are categorical of some kind, I highly recommend reading about categorical data modelling to figure out what to do.

* I think doing one-hot encoding(dummy variable) will help in dealing with the level issues in our predictors. We should also combine similar levels so we can reduce the number of levels. also the negative values which correspond to not in the universe or similar have to be dealt with in someway. we can either choose to omit those observation or try to find a proxy for them. I will prefer the latter.

* For the response variable imbalance we can create a new samples where the distribution between the responses is equal. Maybe something like k-fold Cross-Validation would work but I am not sure and would have to read more about it. 



#### I am going to be adding weights to the data so our minority class, which is food insecurity, affects the model more.


```{r, results='markup', cache = TRUE}
weight_minority_class = sum(FS_Subset$FS_Status == "Food Secure")/sum(FS_Subset$FS_Status == "Food Insecure")
for(i in seq_len(NROW(FS_Subset))){
  
if(FS_Subset$FS_Status[i] == "Food Insecure"){
  FS_Subset$Weight[i] = weight_minority_class

}
  else
    FS_Subset$Weight[i] = 1
}

FS_Subset$Weight <- as.numeric(FS_Subset$Weight)
summary(FS_Subset$Weight)

## Splitting again because train and test doesn't contain the weight column

set.seed(1)

sample <- sample(c(TRUE, FALSE), nrow(FS_Subset), replace = TRUE, prob = c(0.6, 0.4))

train <- FS_Subset[sample, ]
test  <- FS_Subset[!sample, ]

```



```{r, results='markup'}

logistic_weighted <- glm(FS_Status ~ Ethnicity + States + Family_Size + Household_Income:SNAP + Citizenship_status + Number_of_Jobs + Education_Level   , data = train,  weights = Weight, family = "binomial")


```


```{r, results='markup'}

logistic_model2.prob <- predict(logistic_weighted, test, type = "response")
logistic_model2.pred = rep("Food Secure", dim(test)[1])
logistic_model2.pred[logistic_model2.prob > .5] = "Food Insecure"



table(logistic_model2.pred, test$FS_Status)
```
* Our precision for this model is `r 2634/(2634+7540)`

* Our recall for this model is `r 2634/(2634+174)`

* As you can see even though our classification rate of the model went down and the precision of model went down drastically, we were able to raise our recall rate significantly. 

```{r, results='markup'}
test$logistic_model2.prob <- logistic_model2.prob
roc_model2 <- roc(FS_Status ~ logistic_model2.prob, data = test)
plot(roc_model2)
auc(roc_model2)



```


```{r, results='markup'}

train$logistic_model2_train.prob <- predict(logistic_weighted, train, type = "response")

# distribution of the prediction score grouped by known outcome
ggplot( train, aes( logistic_model2_train.prob, color = as.factor(FS_Status) ) ) + 
geom_density( size = 1 ) +
ggtitle( "Training Set's Predicted Score" ) + 
scale_color_economist( name = "data", labels = c( "Food Secure", "Food Insecure" ) ) + 
theme_economist()


```

***

* I think this is enough for logistic regression. We can try changing and adding more predictors, but largely i think this is enough. I wasn't abe to add a lot of important predictors like ethnicity as the model was giving some errors. We should try to figure out how to solve it.

* We are going to try to do random forests and KNN now.

* Working on the data is still our number 1 priority, if we can figure out factor selection and how to use the factors we can't use rn, that would be more helpful than just getting the model to work.

*** 

* Trying to balance data by undersampling 

```{r,results='markup'}

data.balanced.ou <- ovun.sample(FS_Status~., data=train, N=nrow(train), p=0.5,  seed=1, method="both")$data




```

```{r, results = 'markup'}
logistic_both <- glm(FS_Status ~ Ethnicity + States + Family_Size + Household_Income:SNAP + Citizenship_status + Number_of_Jobs + Education_Level   , data = data.balanced.ou,   family = "binomial")

```

```{r, results='markup'}

logistic_model3.prob <- predict(logistic_both, test, type = "response")
logistic_model3.pred = rep("Food Secure", dim(test)[1])
logistic_model3.pred[logistic_model3.prob > .5] = "Food Insecure"



table(logistic_model3.pred, test$FS_Status)
```

```{r, results='markup'}
train$logistic_model3_train.prob <- predict(logistic_both, train, type = "response")
ggplot( train, aes( logistic_model3_train.prob, color = as.factor(FS_Status) ) ) + 
geom_density( size = 1 ) +
ggtitle( "Training Set's Predicted Score" ) + 
scale_color_economist( name = "data", labels = c( "Food Secure", "Food Insecure" ) ) + 
theme_economist()

```

## K-Nearest Neighbours

Here we will try to create a prediction model using KNN Algorithm and we hope to see a model with more accuracy and precision. 

# Preparing Data for KNN

```{r, results='markup'}
str(FS_Subset)
```

```{r, results='markup'}
knn_data <- FS_Subset
knn_data$Family_Size <- as.numeric(FS_Subset$Family_Size)
knn_data$Household_Income <- as.numeric(FS_Subset$Household_Income)
knn_data$Number_of_Jobs <- as.numeric(FS_Subset$Number_of_Jobs)
knn_data$Hours_on_Jobs <- as.numeric(FS_Subset$Hours_on_Jobs)
knn_data$Number_of_children <- as.numeric(FS_Subset$Number_of_children)
knn_data$FS_Status <- as.numeric(FS_Subset$FS_Status)

num_data <- select_if(knn_data, is.numeric)
num_data <- as.data.frame(num_data[-c(7)])
str(num_data)

```

First we want to scale and center the data to ensure KNN will operate correctly.

```{r, results='markup'}
scaledata <- as.data.frame(scale(num_data, center = TRUE, scale = TRUE))
```

We also need to create test and train data sets, we will do this slightly differently by using the sample function. The 2 says create 2 data sets essentially, replacement means we can reset the random sampling across each vector and the probability gives sample the weight of the splits, 2/3 for train, 1/3 for test.

```{r, results='markup'}
set.seed(1000)
knn_sample <- sample(2, nrow(scaledata), replace=TRUE, prob=c(0.67, 0.33))
```
We then just need to use the new variable to create the test/train outputs, selecting the first four rows as they are the numeric data in the iris data set and we want to predict Species.

```{r, results='markup'}
knn_training <- scaledata[knn_sample==1, 1:5]
knn_test <- scaledata[knn_sample==2, 1:5]
```

Now we need to create our Y variables or labels need to input into the KNN function.

```{r, results='markup'}
knn.trainLabels <- num_data[knn_sample==1, 6]
knn.testLabels <- num_data[knn_sample==2, 6]
```

```{r}
# Installing Packages
install.packages("e1071")
install.packages("caTools")
install.packages("class")
  
# Loading package
library(e1071)
library(caTools)
```
```{r}
library(class)
loadPkg("gmodels")
loadPkg("gmodels")
loadPkg("FNN")
loadPkg("caret")
knn.5 <- knn(train=nasa_train, test=nasa_test, cl=nasa_train.labels, k=5)
ACC.5=100 * sum(nasa_test.labels == knn.5)/NROW(nasa_test.labels)
paste0("Accuracy = ", ACC.5)
print(confusionMatrix(knn.5, as.factor(nasa_test.labels) ))


```

# Building Models

```{r, results='markup'}
library(class)
knn_pred <- knn(train = knn_training, test = knn_test, cl=knn.trainLabels, k=3)
knn_pred
```

```{r, results='markup'}
knn_crosst <- gmodels::CrossTable(knn.testLabels, train = knn_pred, prop.chisq = FALSE)
```

```{r, results='markup'}
# create an empty dataframe to store the results from confusion matrices
ResultDf = data.frame( k=numeric(0), Total.Accuracy= numeric(0), row.names = NULL )
kval = 3
knn_pred <- knn(train = knn_training, test = knn_test, cl=knn.trainLabels, k=kval)

knn_crosst <- gmodels::CrossTable(knn.testLabels, train = knn_pred, prop.chisq = FALSE)
print( paste("k = ", kval) )
knn_crosst
```


```{r, results='markup'}
library(caret)
cm <- confusionMatrix(knn_pred, reference = as.factor(knn.testLabels) ) 

# print.confusionMatrix(cm)
# 
cmaccu = cm$overall['Accuracy']
print( paste("Total Accuracy = ", cmaccu ) )

# print("Other metrics : ")
# print(cm$byClass)
# 
cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL ) # initialize a row of the metrics 
# cmt = cbind( cmt, data.frame( t(cm$byClass) ) ) # the dataframe of the transpose, with k valued added in front
ResultDf = rbind(ResultDf, cmt)
print( xkabledply(   as.matrix(cm), title = paste("ConfusionMatrix for k = ",kval ) ) )
```
```{r, results='markup'}
# print("Other metrics : ")
# print(cm$byClass)
# 
cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL ) # initialize a row of the metrics 
# cmt = cbind( cmt, data.frame( t(cm$byClass) ) ) # the dataframe of the transpose, with k valued added in front
ResultDf = rbind(ResultDf, cmt)
print( xkabledply(   as.matrix(cm), title = paste("ConfusionMatrix for k = ",kval ) ) )
```
```{r, results='markup'}
print( xkabledply(data.frame(cm$byClass), title=paste("k = ",kval)) )
```

# Different K-values
```{r, results='markup'}
for (kval in 4:11) {
  knn_pred <- knn(train = knn_training, test = knn_test, cl=knn.trainLabels, k=kval)
  knn_crosst <- CrossTable(knn.testLabels, knn_pred, prop.chisq = FALSE)
  print( paste("k = ", kval) )
  knn_crosst
  # 
  cm = confusionMatrix(knn_pred, reference = as.factor(knn.testLabels )) # from caret library
  # print.confusionMatrix(cm)
  # 
  cmaccu = cm$overall['Accuracy']
  print( paste("Total Accuracy = ", cmaccu ) )
  # print("Other metrics : ")
  # print(cm$byClass)
  # 
  cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL ) # initialize a row of the metrics 
  # cmt = cbind( cmt, data.frame( t(cm$byClass) ) ) # the dataframe of the transpose, with k valued added in front
  ResultDf = rbind(ResultDf, cmt)
  print( xkabledply(   as.matrix(cm), title = paste("ConfusionMatrix for k = ",kval ) ) )
  print( xkabledply(data.frame(cm$byClass), title=paste("k = ",kval)) )
}
```
```{r, results='markup'}
xkabledply(ResultDf, "Total Accuracy Summary")
```
